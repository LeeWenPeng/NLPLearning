# n 元模型

## 1 基础知识了解

**语言模型**

给定文本序列 $x_1,...,x_T$，语言模型的目标是估计联合概率$p(x_1,...,x_T)$

**使用计数来建模**

$p(x,x') = p(x)p(x'|x) = \frac {n(x)}{n} \frac{n(x,x')}{n(x)}$

+   n 为 corpus 中出现的总词数
+   n(x) corpus 中 x 出现的总次数
+   n(x, x') corpus 中 (x, x') 子序列出现的次数

推广到三元

$p(x,x',x'') = p(x)p(x'|x)p(x''|x, x') = \frac {n(x)}{n} \frac{n(x,x')}{n(x)} \frac {n(x, x, x')}{n(x,x')}$

## 2 n 元语法

**马尔可夫假设**

一元语法(1-gram): 假设每一个 x 之间是独立的
$$
\begin{split}
p(x_1, x_2, x_3, x_4) &= p(x_1)p(x_2)p(x_3)p(x_4)\\
&= \frac {n(x_1)}{n} \frac {n(x_2)}{n} \frac {n(x_3)}{n} \frac {n(x_4)}{n} 
\end{split}
$$


二元语法(2-gram): 假设每一个 $x_i$ 只依赖前一个 $x_{i-1}$
$$
\begin{split}
p(x_1, x_2, x_3, x_4) &= p(x_1)p(x_2|x_1)p(x_3|x_2)p(x_4|x_3)\\
&= \frac {n(x_1)}{n} \frac {n(x_1,x_2)}{n(x_1)} \frac {n(x_2,x_3)}{n(x_2)} \frac {n(x_3x_4)}{n(x_3)} 
\end{split}
$$


三元语法(3-gram): 假设每一个 $x_i$ 最多只依赖前两个 $x_{i-1},x_{i-2}$
$$
\begin{split}
p(x_1, x_2, x_3, x_4) &= p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)p(x_4|x_2,x_3)\\
&= \frac {n(x_1)}{n} \frac {n(x_1,x_2)}{n(x_1)} \frac {n(x_1,x_2,x_3)}{n(x_1,x_2)} \frac {n(x_2,x_3x_4)}{n(x_2,x_3)} 
\end{split}
$$


## 3 代码实现

代码

```python
import random
import torch
from d2l import torch as d2l
from matplotlib import pyplot as plt

"""
1 初始化词汇表
"""
tokens = d2l.tokenize(d2l.read_time_machine())
corpus = [token for line in tokens for token in line]
vocab = d2l.Vocab(corpus)
# print(vocab.token_freqs[:10])

def draw_freqs(vocab):
    """
    画词频图
    :param vocab:
    :return:
    """

    plt.title("draw_freqs")
    plt.xlabel("token: x")
    plt.ylabel("frequency: n(x)")
    plt.xscale("log")
    plt.yscale("log")
    if len(vocab)<=1 or not isinstance(vocab, (list, tuple)):
        freqs = [freq for word, freq in vocab.token_freqs]
        plt.plot(freqs)

    for vocab_i in vocab:
        freqs = [freq for word, freq in vocab_i.token_freqs]
        plt.plot(freqs)
    plt.show()

# draw_freqs(vocab)

"""
2 输出2元模型

zip([iterable, ...])
1. zip() 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。
2. 参数：一个或多个迭代器
3. 返回值：元组列表
4. 返回列表长度为参数中最短列表长度
5. 例子： a = [1, 2, 3] b = [4, 5, 6, 7]，则 zip[a, b] = [[1, 4], [2, 5], [3, 6]] 与短列表 a 的长度一样
"""
bigram_tokens = [pair for pair in zip(corpus[:-1], corpus[1:])]
bigram_vocab = d2l.Vocab(bigram_tokens)
bigram_freqs = bigram_vocab.token_freqs
# print(bigram_freqs[:10])

"""
3元模型
"""
trigram_tokens = [triple for triple in zip(corpus[:-2], corpus[1:-1], corpus[2:])]
trigram_vocab = d2l.Vocab(trigram_tokens)
trigram_freqs = trigram_vocab.token_freqs
# print(trigram_freqs[:10])

# draw_freqs((vocab, bigram_vocab, trigram_vocab))
```



## 4 小批量读取

代码

```python
import random
import torch
from d2l import torch as d2l
from matplotlib import pyplot as plt

"""
两种抽样生成小批量序列的方式，目标是实现 n 元语法，这个 n 对应的时间跨度 T，也是 num_steps，也就是根据前面长为 T 的子序列，预测下一个词

所以这里生成的 X 是长度为 T ，也就是 num_steps 的子序列下标列表
而，Y 则是 X 中下标加 1 的值，比如，seq_data_iter_random(my_seq, batch_size=2, num_steps=5)中生成的一组

X: tensor([[23, 24, 25, 26, 27],
        [28, 29, 30, 31, 32]]) 
Y: tensor([[24, 25, 26, 27, 28],
        [29, 30, 31, 32, 33]])
        
这里实际的意义是，如果给的数据为 [23]，则需要预测 24；如果给的数据为[23, 24]，则需要预测 25，如果给的数据为[23, 24, 25]，则需要预测 26
也就是最多根据 num_steps 个长度的数据去预测下一个数据，也就是为了 n-gram 做准备

生成小批量方法的参数 corpus 是 list of tokens` idx
"""
def seq_data_iter_random(corpus, batch_size, num_steps):
    """
    使用随机抽样生成一个小批量子序列
    corpus: 词汇表
    batch_size: 批量大小
    num_steps: 子序列长度 32 128 512

    流程：
    1. 设置起始位置 k，也就是取 corpus[k:]
    2. 将所有子序列起始位置存到列表 initial_indices 中
    3. 将 initial_indices 中的数据打乱
    4. 定义读取子序列的函数，也就是当我们输入子序列起始位置输入时，能够得到完整子序列
    5. 根据 batch_size 进行分批
    """
    corpus = corpus[random.randint(0, num_steps - 1):] # 随机设置一个起点 起点 k 在[0, num_steps - 1]，将 k 之前的序列扔掉
    num_subseqs = (len(corpus) - 1)  // num_steps # 生成多少个子序列
    initial_indices = list(range(0, num_subseqs * num_steps, num_steps)) # 生成每个子序列开始的下标 pos
    random.shuffle(initial_indices) # 打乱序列下标

    def data(pos): # 根据 pos 读取子序列段
        return corpus[pos: pos + num_steps]

    num_batches = num_subseqs // batch_size
    for i in range(0, batch_size * num_batches, batch_size): # 生成小批量 之所以终点是 batch_size * num_batches，是为了舍弃最后数据最后不足一个 batch_size 大小的尾段
        initial_indices_per_batch = initial_indices[i: i+batch_size] # 每一个批量中的子序列开始下标
        X = [data(j) for j in initial_indices_per_batch] # 根据每一个批量中的子序列开始下标生成批量数据
        Y = [data(j+1) for j in initial_indices_per_batch] # 同上
        yield torch.tensor(X), torch.tensor(Y) # 使用 yield 生成tensor迭代器

my_seq = list((range(35)))
for X, Y in seq_data_iter_random(my_seq, 2, 5):
    print("X:", X, "\nY:", Y)
    break

def seq_data_iter_sequential(corpus, batch_size, num_steps):
    """
    使用随机抽样生成一个小批量子序列

    过程：
    1. 设置起始位置 k
    2. 设置 Xs, Ys = corpus[k:], corpus[k+1:]
    3. 将 Xs，Ys 分批
    4. 切分子序列

    可以看出来，上一个随机抽样方法是先分子序列，再将子序列分批
    而这个方法是先分批再分子序列，效果是一样的5
    """
    offset = random.randint(0, num_steps) # k， 也就是起始值，舍弃 k 之前的值
    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size # 舍弃最后一部分
    Xs = torch.tensor(corpus[offset: offset+num_tokens]) # 转换为 tensor
    Ys = torch.tensor(corpus[offset+1: offset+1+num_tokens])
    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1) # 将 tensor 等分成 batch_size 份，每一份都是一行
    num_batches = Xs.shape[1] // num_steps # 计算每一个batch 能够分成多少个子序列
    for i in range(0, num_steps * num_batches, num_batches): # 将 Xs 和 Ys 的tensor，根据 num_steps 分成每一个 X，Y
        X = Xs[:, i:i+num_steps]
        Y = Ys[:, i:i+num_steps]
        yield  X, Y

my_seq = list((range(35)))
for X, Y in seq_data_iter_sequential(my_seq, 2, 5):
    print(X, "\n", Y)

class SeqDataLoader:
    """加载序列数据的迭代器"""
    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):
        if use_random_iter:
            self.data_iter_fn = d2l.seq_data_iter_random
        else:
            self.data_iter_fn = d2l.seq_data_iter_sequential
        self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens) # 这里的corpus是文本下标，是 vocab 调用了 __getitem__ 方法， 也就是 tokens_to_idx
        self.batch_size, self.num_steps = batch_size, num_steps

    def __iter__(self):
        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)

def load_data_time_machine(batch_size, num_steps, use_random_iter=False, max_tokens=10000):
    """返回时光机器的迭代器和数据集"""
    data_iter = SeqDataLoader(batch_size, num_steps, use_random_iter, max_tokens)
    return data_iter, data_iter.vocab
#
# data_iter, data_iter.vocab = load_data_time_machine(2,5)
# for x, y in data_iter:
#     print(x, "\n", y)
#     break
```

## 5 实现一个简单的序列模型

代码

```python
import torch
from torch import nn
from d2l import torch as d2l
from matplotlib import pyplot as plt


def build_data(T):
    """根据 T 生成序列数据"""
    time = torch.arange(1, T + 1, dtype=torch.float32)
    x = torch.sin(0.01 * time) + torch.normal(0, 0.2, (T,))
    return time, x


"""1 生成序列数据"""
T = 1000
time, x = build_data(T)


def draw_train_data(time, x):
    """绘制训练数据图"""
    # help(d2l.plot)
    plt.figure(figsize=(6, 3))
    plt.xlabel('time')
    plt.ylabel('x')
    # plt.xlim(1, 1000)
    plt.figure(num=1, figsize=(6, 3))
    plt.grid(True)
    plt.plot(time, x, '-')
    plt.show()


"""绘图"""


# draw_train_data(time, x)


def build_features_and_labels(x, T, tau):
    """
    将训练数据转换成模型 特征 - 标签对
    tau 是 n-grim 的 n
    比如这里的 tau 为 4
    也就是说
    y_i = x_i
    对应 y_i 的 特征 X_i = [x_{i-4}, x_{i-3}, x_{i-2}, x_{i-1}]
    需要 x_i 前四个数据才能预测出当前的 x_i
    """
    features = torch.zeros(T - tau, tau)
    for i in range(tau):
        features[:, i] = x[i: T - tau + i]
    lables = x[tau:].reshape(-1, 1)
    return features, lables


tau = 4
features, labels = build_features_and_labels(x, T, tau)


def build_train_iter(features, labels, batch_size, n_train):
    # 只有前 n_train 个用于训练
    return d2l.load_array((features[:n_train], labels[:n_train]), batch_size, is_train=True)


batch_size, n_train = 16, 600
train_iter = build_train_iter(features, labels, batch_size, n_train)

"""
模型： 一个拥有两个全连接层的多层感知机，ReLU激活函数和平方损失。
"""


def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight)


def get_net():
    net = nn.Sequential(
        nn.Linear(4, 10),
        nn.ReLU(),
        nn.Linear(10, 1)
    )
    net.apply(init_weights)
    return net


loss = nn.MSELoss(reduction='none')


def train(net, train_iter, loss, epochs, lr):
    trainer = torch.optim.Adam(net.parameters(), lr)
    for epoch in range(epochs):
        for X, y in train_iter:
            trainer.zero_grad()
            l = loss(net(X), y)
            l.sum().backward()
            trainer.step()
        print(f'epoch: {epoch + 1}, '
              f'loss: {d2l.evaluate_loss(net, train_iter, loss):f}')

net = get_net()
epochs, lr = 5, 0.01
train(net, train_iter, loss, epochs, lr)
# epoch: 1, loss: 0.061326
# epoch: 2, loss: 0.050311
# epoch: 3, loss: 0.049126
# epoch: 4, loss: 0.047366
# epoch: 5, loss: 0.048936


"""
下列是预测部分
"""


onestep_preds = net(features)


def draw_labels_and_features(time, x ,onestep_preds):
    plt.figure(figsize=(6, 3))
    plt.xlabel('time')
    plt.ylabel('x')
    # plt.xlim(1, 1000)
    plt.figure(num=1, figsize=(6, 3))
    plt.grid(True)
    plt.plot(time, x.detach().numpy(), '-')
    plt.plot(time[tau:], onestep_preds.detach().numpy(), '-')
    plt.show()


multistep_preds = torch.zeros(T)
multistep_preds[: n_train + tau] = x[:n_train + tau]
for i in range(n_train + tau, T):
    multistep_preds[i] = net(multistep_preds[i - tau: i].reshape(1, -1))


def draw_labels_and_features_multisetp_preds(time, x ,multistep_preds, onestep_preds):
    plt.figure(figsize=(6, 3))
    plt.xlabel('time')
    plt.ylabel('x')
    # plt.xlim(1, 1000)
    plt.figure(num=1, figsize=(6, 3))
    plt.grid(True)
    plt.plot(time, x.detach().numpy(), '-')
    plt.plot(time[tau:], onestep_preds.detach().numpy(), '-')
    plt.plot(time[n_train + tau:], multistep_preds[n_train+tau:].detach().numpy(), '-.')
    plt.show()


# draw_labels_and_features_multisetp_preds(time, x, multistep_preds, onestep_preds)
"""
# 多步预测的效果很差，经过几个预测步骤之后，就会衰减成为一个常数

原因：错误累积
"""

max_steps = 64

features = torch.zeros(T - tau - max_steps + 1, tau + max_steps)
for i in range(tau):
    features[:, i] = x[i:i + T - tau - max_steps + 1]
for i in range(tau, tau+max_steps):
    features[:, i] = net(features[:, i - tau:i]).reshape(-1)

steps = (1, 4, 16, 64)

plt.figure(figsize=(6, 3))
plt.xlabel('time')
plt.ylabel('x')
plt.xlim(5, 1000)
plt.figure(num=1, figsize=(6, 3))
plt.grid(True) # 网格

plt.plot(time[tau + 1 - 1: T-max_steps + 1], features[:, tau + 1 - 1].detach().numpy(), '-', label='1_step')
plt.plot(time[tau + 4 - 1: T-max_steps + 4], features[:, tau + 4 - 1].detach().numpy(), '--', label='4_step')
plt.plot(time[tau + 16 - 1: T-max_steps + 16], features[:, tau + 16 - 1].detach().numpy(), '-.', label='16_step')
plt.plot(time[tau + 64 - 1: T-max_steps + 64], features[:, tau + 64 - 1].detach().numpy(), ':', label='64_step')
plt.legend()
plt.show()

"""
序列模型的估计需要专门的统计工具，两种较流行的选择是自回归模型和隐变量自回归模型。

对于时间是向前推进的因果模型，正向估计通常比反向估计更容易。

对于直到时间步 t 的观测序列，其在时间步 t+k 的预测输出是“ k 步预测”。随着我们对预测时间 k 值的增加，会造成误差的快速累积和预测质量的极速下降。
"""
```

