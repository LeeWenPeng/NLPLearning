# 62 序列到序列学习

## 1 应用范畴

1. DNA 转录
2. 机器翻译

## 2 机器翻译
![Alt text](image-1.png)

+ 编码器是一个RNN
  + 这个RNN可以是双向的
+ 解码器是另一个RNN

### 2.1 细节：隐藏状态的传递

![Alt text](image-2.png)

+ 编码器将最后的隐藏状态作为解码器的初始隐藏状态，和句子嵌入链接，然后共同作为编码器输入
+ 编码器不需要输出，只需要获得最后的隐藏状态就行

### 2.2 训练

解码器的训练，使用目标句子作为输入

![Alt text](image-3.png)

> 上图中进行的推理，就是在神经网络训练结束后，开始实际工作时进行的操作
> 因为知道目标结果，所以在训练时，就可以利用一个小技巧，就是
> 1. 首先我训练编码器时，先将源句子的嵌入作为输入，输入进去
> 2. 然后，模型运行到解码器阶段时，我这里不用从编码器那里获得的隐藏状态了，因为其不一定是对的，我这里直接使用目标句子的嵌入作为输入

### BLEU

用于衡量生成非定长序列文本的好坏

![Alt text](image-4.png)

+ $p_n$ 就是预测中，所有 n-gram 的精度
  + $p_n$ 的下标 n 就是 n-gram 的 n
  + 比如标签序列为 A B C D E F, 预测序列为 A B B C D
  + 那么 $p_1$ 就是所有预测序列中 1-gram 的精度，正确预测到的 1-gram 为 A B C D，正确值为4，总为5，也就是 4/5
  + $p_2$ 就是所有预测序列中的 2-gram 的精度，正确预测到的 2-gram 为 AB BC CD，正确值为3，总为AB BB BC CD 4，也就是3/4
  + $p_3$ 就是 ABB BBC BCD 三个 3-gram 中正确的比例，显然只有 BCD 一个，就是 1/3
  + $p_4$ 则为 0

+ 公式的前半截 $\exp(\min(0, 1-\frac{len_{\text{label}}}{len_{\text{pred}}}))$ 的作用为**惩罚过短的预测**
  + 如果预测的长度比标签长度短很多，min 取到的值就是个负数，e 的指数为负就会变成一个很小的数

+ 公式的后半截 $p_n$ 的指数的作用为**增加长序列的权重**
  + 首先$p_n$为小于 1 的数，那么，权重越大，结果就会越少，所以，这里给的 $\frac{1}{2^n}$ 就是当预测长度越长时，使得结果越大
  + 其次，如果惩罚长预测，就是说，看到上一条后，难免会有疑问，如果只是奖励长序列，那么不是只要预测序列越长，就价值越高？
    1. 显而易见，我们这里不能忽略掉 $p_n$ 本身，其本身就已经有了惩罚长序列的特质
    2. 通过上面对 $p_n$ 的阐述，能够看出当 n 越大时，$p_n$ 越小
    3. 当 $n > len_{\text{label}}$ 时 $p_n$ 更是直接为 0

## 3 总结

![image.png](main_files/image.png)

## 4 实现

### 4.1 引入依赖


```python
import collections
import math
import torch
from torch import nn
from d2l import torch as d2l
```

### 4.2 实现循环神经网络编码器


```python
class Seq2seqEncoder(d2l.Encoder):
    """用于序列到序列学习的循环神经网络编码器"""

    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs):
        super(Seq2seqEncoder, self).__init__(**kwargs)

        # 嵌入层
        # 输入是 vocab_size 输出 embed_size
        self.embedding = nn.Embedding(vocab_size, embed_size)
        # 初始化神经网络为 GRU，输入是embed_size，输出是 num_hiddens * num_layers
        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers, dropout=dropout)

    def forward(self, X, *args):

        # 输出'X'的形状：(batch_size, num_steps, embed_size)
        X = self.embedding(X)
        # 在循环神经网络中第一个轴对应的是时间步
        # 所以把 num_steps 放到前面，把 batch_size 放到中间
        X = X.permute(1, 0, 2)
        output, state = self.rnn(X)
        # output的形状:(num_steps,batch_size,num_hiddens)
        # state的形状:(num_layers,batch_size,num_hiddens)
        return output, state
```


```python
encoder = Seq2seqEncoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)
encoder .eval()
# batch_size 4
# num_steps 7
X = torch.zeros((4, 7), dtype=torch.long)
output, state = encoder(X)
# output的形状:(num_steps,batch_size,num_hiddens)
output.shape
```




    torch.Size([7, 4, 16])




```python
# state的形状:(num_layers,batch_size,num_hiddens)
state.shape
```




    torch.Size([2, 4, 16])



## 4.3 实现循环神经网络解码器


```python
class Seq2SeqDecoder(d2l.Decoder):
    """用于序列对序列学习的循环神经网络的解码器"""
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,  dropout=0,**kwargs):
        super(Seq2SeqDecoder, self).__init__(**kwargs)
        # 这里的 embedding 不能和编码器的一样，vocab_size不同，比如编码器针对的是英语，而解码器针对的是法语
        self.embedding = nn.Embedding(vocab_size, embed_size)
        # self.rnn = nn.GRU(embed_size, num_hiddens, num_layers, dropout=dropout)
        # 注意第一个参数值为 embed_size + num_hiddens 说明给定的输入为 X + num_hiddens
        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,
                          dropout=dropout)
        # 输出层
        self.dense = nn.Linear(num_hiddens, vocab_size)

    def init_state(self, encoder_outputs, *args):
        # encoder_outputs 中间包含 encoder_output 和 encoder_state
        # 这里只需要 encoder_state
        return encoder_outputs[1]

    def forward(self, X, state):
        # X shape num_steps * batch_size * embed_size
        X = self.embedding(X).permute(1, 0, 2)
        # state[-1] 将encoder 输出的最后一层拿出来，重复 num_steps 次
        # content num_steps
        content = state[-1].repeat(X.shape[0], 1, 1)
        X_and_content = torch.cat((X, content), 2)
        output, state = self.rnn(X_and_content, state)
        # 将 batch_size 重新放到前面
        output = self.dense(output).permute(1, 0, 2)
        # output shape  batch_sizes, num_steps, vocab_size
        # state shape num_layers, batch_sizes, num_hiddens
        return output, state
```

实例化


```python
decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)
decoder.eval()
# output = encoder(X)
state = decoder.init_state(encoder(X))
output, state = decoder(X, state)
output.shape, state.shape
# output torch.Size([4, 7, 10]) 4 batch_size 7 num_steps 10 vocab_size 对每一个样本在每一个时刻做一个输出
# state torch.Size([2, 4, 16])) 2 num_layers4 batch_size 16 num_hiddens 在每一层对于每个样本都计算出隐藏状态
```




    (torch.Size([4, 7, 10]), torch.Size([2, 4, 16]))



## 4.4 损失函数
### 4.4.1 遮蔽函数


```python
def sequence_mask(X, vaild_len, value=0):
    """ 作用： 在序列中屏蔽不相关的项
        X 输入 是一个 n*m tensor
        vaild_len n tensor 其中的元素表示X针对于第 1 个轴需要保留多少第 2 个轴的元素
                            比如 :
                            X = tensor([[1, 2, 3], 
                                        [4, 5, 6]])
                            vaild_len = [1, 2] 表示第一个样本中保留 1 个元素，第二个样本中保留 2 个元素
                            所以 X 的最后结果为 tensor([[1, 0, 0],
                                                    [4, 5, 0]])
        value 需要替换的 mask 值，比如上述例子中的原值都被替换成了 0

        因为代码 X[~mask] 就导致 要求 mask 和 X 的 shape 必须一样
        而 mask 第一个轴的长度是 vaild_len.size(0) 决定的
        torch.arange((maxlen), dtype=torch.float32, device=X.device)[None,: ] 控制 mask 的第二轴数，一定是与 X 相等的
    """
    # X.size() = X.shape; 
    # X.size(0) 就是行数;
    # X.size(1) 就是列数；
    maxlen=X.size(1) 
    # 下行代码利用传播技术
    # torch.arange((maxlen), dtype=torch.float32, device=X.device)[None,: ]
    # value: tensor([[0., 1., 2.]])
    # 
    # vaild_len[:,None]
    # value: tensor([[1], 
    #               [2]])
    # 
    # tensor([[0., 1., 2.]]) < [1] = tensor([True, False, False])
    # tensor([[0., 1., 2.]]) < [2] = tensor([True,  True, False])
    #
    # 所以结果也就是 mask = tensor([[ True, False, False], 
    #                               [ True,  True, False]])
    mask = torch.arange((maxlen), dtype=torch.float32, device=X.device)[None,: ] < vaild_len[:, None]
    # ~ 取反运算符 这里同样是传播技术 ~mask = tensor([[ False, True, True], 
    #                                               [ False,  False, True]])
    # X[~mask] 也就是显而易见了，将对应为 True 的值替换为 value
    X[~mask] = value
    return X

X = torch.tensor([[1, 2, 3], 
                  [4, 5, 6]])
# vaild_len 1, 2
sequence_mask(X, torch.tensor([1, 2]))
```




    tensor([[1, 0, 0],
            [4, 5, 0]])



**测试**
通过上述函数屏蔽不相关的轴


```python
X = torch.ones(2, 3, 4)
sequence_mask(X, torch.tensor([1, 2]))
```




    tensor([[[1., 1., 1., 1.],
             [0., 0., 0., 0.],
             [0., 0., 0., 0.]],
    
            [[1., 1., 1., 1.],
             [1., 1., 1., 1.],
             [0., 0., 0., 0.]]])




```python
X = torch.ones(3, 3, 4)
sequence_mask(X, torch.tensor([1, 2, 3]))
```




    tensor([[[1., 1., 1., 1.],
             [0., 0., 0., 0.],
             [0., 0., 0., 0.]],
    
            [[1., 1., 1., 1.],
             [1., 1., 1., 1.],
             [0., 0., 0., 0.]],
    
            [[1., 1., 1., 1.],
             [1., 1., 1., 1.],
             [1., 1., 1., 1.]]])



### 4.4.2 带遮蔽的 softmax 交叉熵损失函数


```python
class MaskSoftmaxCELoss(nn.CrossEntropyLoss):
    """带遮蔽的softmax交叉熵损失函数"""
    # pred的形状：(batch_size,num_steps,vocab_size)
    # label的形状：(batch_size,num_steps)
    # valid_len的形状：(batch_size,)
    # 改进的地方就在于引入了 vaild_size
    def forward(self, pred, label, vaild_len):
        weight = torch.ones_like(label)
        # 这里的 weight 就是个 mask tensor：需要保留的值为1 不相关值为0
        weight = sequence_mask(weight, vaild_len) 
        # 设置 reduction 为 none，不计算 min 
        self.reduction = 'none'
        # 这一句是正常的softmax交叉熵损失函数计算
        # python 类定义带的括号，比如 class MaskSoftmaxCELoss(nn.CrossEntropyLoss)
        # 这里 nn.CrossEntropyLoss 就是 MaskSoftmaxCELoss 的基类
        # 换句话说，定义的类继承括号里的类
        # 所以这里的 forward 函数是对子类继承父类forward 函数的一个重载
        # 下面那句代码使用了 super 也就是调用父类的函数
        # 也就变成了正常的 softmax 交叉熵损失函数的计算
        # 
        # 那么 为何要对 pred 进行一个维度调整 也就是 permute 操作
        # 这时 pytorch 的规定，要将预测的维度放到中间，也就是 vocab_size 放到中间位置
        unweighted_loss = super(MaskSoftmaxCELoss, self).forward(pred.permute(0, 2, 1), label)
        # 上句代码可以简化如下：
        # unweighted_loss = super().forward(pred.permute(0, 2, 1), label)
        # 这一句将 mask 加入进去，把不相关的值都变成 0
        # 对每个句子取个平均，对每个样本返回一个 loss
        weighted_loss = (unweighted_loss*weight).mean(dim=1)
        return weighted_loss

```

**测试**


```python
loss = MaskSoftmaxCELoss()
# pred label vaild_len 
# vaild_len.size() = 3 = pred.size(0) = label.size(0)
loss(torch.ones(3, 4, 10), torch.ones((3, 4), dtype=torch.long), torch.tensor([4, 2, 0]))
```




    tensor([2.3026, 1.1513, 0.0000])



## 4.5 EncoderDecoder


```python
class EncoderDecoder(nn.Module):
    """Encoder Decoder 组合"""
    def __init__(self, encoder, decoder, **kwargs):
        super(EncoderDecoder, self).__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder
    
    def forward(self, encoder_X, decoder_X, *args):
        encoder_outputs = self.encoder(encoder_X, *args)
        decoder_init_state = self.decoder.init_state(encoder_outputs, *args)
        return self.decoder(decoder_X, decoder_init_state)

```

## 4.6 训练


```python
"""
 训练函数
 args:
 1. net 神经网络
 2. data_iter 数据
 3. lr 学习率
 4. num_epochs 训练次数
 5. tgt_vocab 
 6. device 设备
"""
def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):
    """训练序列到序列模型"""
    def xavier_init_weights(m):
        if type(m) == nn.Linear:
            nn.init.xavier_uniform_(m.weight)
        if type(m) == nn.GRU:
            for param in m._flat_weights_names:
                if "weight" in param:
                    nn.init.xavier_uniform_(m._parameters[param])

    net.apply(xavier_init_weights)
    net.to(device)
    optimizer = torch.optim.Adam(net.parameters(), lr=lr)
    loss = MaskSoftmaxCELoss() # 设置 loss 为带遮蔽的交叉熵损失函数
    net.train()
    animator = d2l.Animator(xlabel='epoch', ylabel='loss',
                     xlim=[10, num_epochs])
    for epoch in range(num_epochs):
        """ 核心代码 """
        timer = d2l.Timer()
        metric = d2l.Accumulator(2)  # 训练损失总和，词元数量
        for batch in data_iter:
            optimizer.zero_grad()
            # seq2seq 训练时与普通 RNN 不同之处,也就是构造 decoder 的输入
            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch] # 从 batch 中取出 源句子 和 目标句子
            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0], # begin of sentence 创建 <bos>
                          device=device).reshape(-1, 1)
             # 强制教学 
             # 将 <bos> 与 Y[:, :-1] 拼接到一起
             # Y[:, :-1] 也就是第一个词到倒数第二个词
             # 因为预测的时候,是上一个词预测下一个词,所以将目标句子的最后一个词去掉,并整体后移一位,就能够变成预测的label
             # 那么第一个词,就需要一个标签来填位,也就是 <bos> 
            dec_input = torch.cat([bos, Y[:, :-1]], 1) 
            # 这里的参数为 
            # 1. X 源句子
            # 2. dec_input 目标句子,这里用作 decoder的输入
            #              原因已经在上文讲过了, 这里回忆一下
            #               因为这个模型是试图使用输入的 X 去预测到 Y 的
            #               然后 encoder 是将源文本 X 计算出 enc_output 和 enc_state
            #               decoder 是将 enc_state 计算出目标文本 Y
            #               那么这里就直接使用 Y 修改后的 dec_input 作为 enc_state 输入到 decoder 中
            Y_hat, _ = net(X, dec_input, X_valid_len) # 神经网络计算预测值y_hat
            l = loss(Y_hat, Y, Y_valid_len) # 求损失

            l.sum().backward()      # 损失函数的标量进行“反向传播” 求导
            d2l.grad_clipping(net, 1) # 梯度裁剪 theta = 1
            num_tokens = Y_valid_len.sum() 
            optimizer.step()
            with torch.no_grad():
                metric.add(l.sum(), num_tokens)

        if (epoch + 1) % 10 == 0:
            animator.add(epoch + 1, (metric[0] / metric[1],))
    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '
        f'tokens/sec on {str(device)}')
    

```

**创建和训练一个序列到序列模型**

数据集使用机器翻译数据集


```python
embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1
batch_size, num_steps = 64, 7
# 这里 num_steps 应该是最普遍的长度中的中位数，可以求期望，也就是将 sum(len(Sentence_i) * count(Sentence_i))/count(Sentence) 
lr, num_epochs, device = 0.005, 300, d2l.try_gpu()

train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)
encoder = Seq2seqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers,
                        dropout)
decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers,
                        dropout)
net = EncoderDecoder(encoder, decoder)
train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)
```

    loss 0.027, 30300.5 tokens/sec on cuda:0
    


    
![svg](main_files/main_27_1.svg)
    


## 4.7 预测


```python
def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,
                    device, save_attention_weights=False):
    """序列到序列模型的预测"""
    # 在预测时将net设置为评估模式
    # 所谓评估模式，就是将网络的训练功能停止，所有参数固定
    # 因为这里是预测，net已经是训练后的模型了，后续做预测时不应该再对模型中的参数做调整
    # 所以这里将 net 设置为评估模式，net 中的参数都使用训练出的模型参数
    net.eval()

    # 给源句子添加结束标记 <eos>
    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [
        src_vocab['<eos>']] 
    enc_valid_len = torch.tensor([len(src_tokens)], device=device)
    # 将源句子截断或者填充为 num_steps 长度
    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>']) 
    # 添加批量轴 批量就是 batch，这个操作将 src_tokens 包成了一个 tensor
    # 之所以要添加批量轴是为了将维度对齐，之前的操作都有batch这个维度，所以这里需要添加 batch 这个轴
    enc_X = torch.unsqueeze(
        torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)
    enc_outputs = net.encoder(enc_X, enc_valid_len)
    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)
    # 添加批量轴
    dec_X = torch.unsqueeze(torch.tensor(
        [tgt_vocab['<bos>']], dtype=torch.long, device=device), dim=0)
    output_seq, attention_weight_seq = [], []
    # 预测 n 步
    for _ in range(num_steps):
        Y, dec_state = net.decoder(dec_X, dec_state)
        # 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入
        # 上述代码中，截止到上一句代码，已经得到之前训练中 decoder 计算出的 output, state
        # 这里将 decoder 计算出来的 output,state 作为下一步预测的一个输入
        dec_X = Y.argmax(dim=2)
        # 降维，作用是为了加快计算，这个操作会将所有的没有用的维度，比如上述扩充的 batch 的维度给去除掉
        # 由于上述扩充的 batch 维度是在 dim=0 这个维度上的，这里将为同样作用于这个维度
        pred = dec_X.squeeze(dim=0).type(torch.int32).item()
        # 保存注意力权重（稍后讨论）
        if save_attention_weights:
            attention_weight_seq.append(net.decoder.attention_weights)
        # 一旦序列结束词元被预测，输出序列的生成就完成了
        if pred == tgt_vocab['<eos>']:
            break
        output_seq.append(pred)
    return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq
```


```python
def predict_seq2seq_1(net, src_sentence, src_vocab, tgt_vocab, num_steps,
                    device, save_attention_weights=False):
    """序列到序列模型的预测"""
    # 在预测时将net设置为评估模式
    # 所谓评估模式，就是将网络的训练功能停止，所有参数固定
    # 因为这里是预测，net已经是训练后的模型了，后续做预测时不应该再对模型中的参数做调整
    # 所以这里将 net 设置为评估模式，net 中的参数都使用训练出的模型参数
    net.eval()

    # 给源句子添加结束标记 <eos>
    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [
        src_vocab['<eos>']] 
    enc_valid_len = torch.tensor([len(src_tokens)], device=device)
    # 将源句子截断或者填充为 num_steps 长度
    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>']) 
    # 添加批量轴 批量就是 batch，这个操作将 src_tokens 包成了一个 tensor
    # 之所以要添加批量轴是为了将维度对齐，之前的操作都有batch这个维度，所以这里需要添加 batch 这个轴
    enc_X = torch.unsqueeze(
        torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)
    enc_outputs = net.encoder(enc_X, enc_valid_len)
    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)
    # 添加批量轴
    dec_X = torch.unsqueeze(torch.tensor(
        [tgt_vocab['<bos>']], dtype=torch.long, device=device), dim=0)
    output_seq, attention_weight_seq = [], []
    # 预测 n 步
    for _ in range(num_steps):
        Y, dec_state = net.decoder(dec_X, dec_state)
        # 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入
        # 上述代码中，截止到上一句代码，已经得到之前训练中 decoder 计算出的 output, state
        # 这里将 decoder 计算出来的 output,state 作为下一步预测的一个输入
        dec_X = Y.argmax(dim=2)
        # 降维，作用是为了加快计算，这个操作会将所有的没有用的维度，比如上述扩充的 batch 的维度给去除掉
        # 由于上述扩充的 batch 维度是在 dim=0 这个维度上的，这里将为同样作用于这个维度
        pred = dec_X.squeeze(dim=0).type(torch.int32).item()
        # 保存注意力权重（稍后讨论）
        if save_attention_weights:
            attention_weight_seq.append(net.decoder.attention_weights)
        # 一旦序列结束词元被预测，输出序列的生成就完成了
        if pred == tgt_vocab['<eos>']:
            break
        output_seq.append(pred)
    return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq
```

## 4.8 BELU 代码实现

这里回顾一下 BELU 的公式

![Alt text](image-4.png)


```python
"""
args:
+ pred_seq 预测序列
+ label_seq 标签序列
+ k 前 k 步，计算分数的步数
"""
def bleu(pred_seq, label_seq, k):  #@save
    """计算BLEU"""
    pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')
    len_pred, len_label = len(pred_tokens), len(label_tokens)
    # 这里这个分数就是公式前部分的公式
    score = math.exp(min(0, 1 - len_label / len_pred))
    for n in range(1, k + 1):
        # 初始化匹配次数和子序列列表
        num_matches, label_subs = 0, collections.defaultdict(int)
        for i in range(len_label - n + 1):
            # 更新子序列列表
            label_subs[' '.join(label_tokens[i: i + n])] += 1
        for i in range(len_pred - n + 1):
            # 更新匹配次数
            if label_subs[' '.join(pred_tokens[i: i + n])] > 0:
                num_matches += 1
                label_subs[' '.join(pred_tokens[i: i + n])] -= 1
        # 这里的公式就是后半部分
        score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))
    return score
```


```python
engs = ['go .', "i lost .", 'he\'s calm .', 'i\'m home .']
fras = ['va !', 'j\'ai perdu .', 'il est calme .', 'je suis chez moi .']
for eng, fra in zip(engs, fras):
    translation, attention_weight_seq = predict_seq2seq(
        net, eng, src_vocab, tgt_vocab, num_steps, device)
    print(f'{eng} => {translation}, bleu {bleu(translation, fra, k=2):.3f}')
```

    go . => va !, bleu 1.000
    i lost . => j'ai perdu ., bleu 1.000
    he's calm . => il est paresseux ., bleu 0.658
    i'm home . => je suis chez moi <unk> ., bleu 0.803
    
