{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自然语言处理\n",
    "\n",
    "文本处理工具：`spaCy`和`NLTK`\n",
    "> 下面都使用`spacy`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 文本分词\n",
    "\n",
    "将文本分解为词的过程\n",
    "\n",
    "对于语句的处理分为两种\n",
    "\n",
    "1. 对于英语汉语这种比较容易分词的语言采用分词操作\n",
    "2. 对于黏着语言，直接将文本表示为一个字节流\n",
    "\n",
    "分词：\n",
    "+ 输入：文本或标记\n",
    "+ 输出：词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mary', ',', 'do', \"n't\", 'slap', 'the', 'green', 'witch']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# nlp = spacy.load('en') # 已经被淘汰, 现在使用下面那种形式\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "txt = \"Mary, don't slap the green witch\"\n",
    "print([str(token) for token in nlp(txt.lower())])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 n元模型\n",
    "\n",
    "有n个词的序列\n",
    "> 这里的词，并不是指一个完整的单词，可以是单词提取出来的词干词缀等\n",
    "> \n",
    "> 所以, n元模型可以不只是表示单词序列，也可以表示单个单词等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mary', ',', 'do'], [',', 'do', \"n't\"], ['do', \"n't\", 'slap'], [\"n't\", 'slap', 'the'], ['slap', 'the', 'green'], ['the', 'green', 'witch']]\n"
     ]
    }
   ],
   "source": [
    "def n_grams(txt, n):\n",
    "    \"\"\"\n",
    "    输入token或text, 输出n元模型的列表\n",
    "    \"\"\"\n",
    "    return [txt[i : i + n] for i in range(len(txt) - n + 1)]\n",
    "\n",
    "\n",
    "cleaned = [str(token) for token in nlp(txt.lower())]\n",
    "print(n_grams(cleaned, 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 词形还原和词干提取\n",
    "\n",
    "词形还原：将词都转换为词根的形式\n",
    "> 一个需要理解语言形态学的机器学习问题\n",
    ">\n",
    "> spaCy 使用预定义词典`WordNet`进行词形还原\n",
    "\n",
    "词干提取：词形还原的一种，使用手工制作的规则，通过去除单词词缀，将单词简化为词干"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he-->he\n",
      "was-->be\n",
      "running-->run\n",
      "late-->late\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"he was running late\")\n",
    "for token in doc:\n",
    "    print(\"{}-->{}\".format(token, token.lemma_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 词性标注\n",
    "\n",
    "1. 文档分类\n",
    "\n",
    "对文档进行分类\n",
    "\n",
    "可以使用`TF`和`TF-IDF`\n",
    "\n",
    "2. 单词分类\n",
    "\n",
    "文档分类概念的延伸\n",
    "\n",
    "可以使用**词性标注**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary --> PROPN\n",
      "slapped --> VERB\n",
      "the --> DET\n",
      "green --> ADJ\n",
      "witch --> NOUN\n",
      ". --> PUNCT\n"
     ]
    }
   ],
   "source": [
    "# 词性标注\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"Mary slapped the green witch.\")\n",
    "for token in doc:\n",
    "    print(\"{} --> {}\".format(token, token.pos_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 广度分类：分块和命名实体识别\n",
    "\n",
    "广度类型：短语或者具有特定含义的词\n",
    "\n",
    "广度分类就是识别出文本中的广度类型，或者说是**识别文本中广度类型数据的词性**\n",
    "1. 分块/浅层解析\n",
    "   1. 深度学习\n",
    "   2. 正则表达式\n",
    "2. 命名实体识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary --> NP\n",
      "the green witch --> NP\n"
     ]
    }
   ],
   "source": [
    "# 名词短语分块\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Mary slapped the green witch.\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(\"{} --> {}\".format(chunk, chunk.label_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 句子结构\n",
    "\n",
    "浅层句法分析来识别短语词性，句法分析**识别短语之间的关系**\n",
    "\n",
    "句法分析包括两种：\n",
    "1. **成分句法分析**\n",
    "2. **依存句法分析**\n",
    "\n",
    "句法分析：\n",
    "+ 输入：句子\n",
    "+ 输出：句法分析树"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词义与语义\n",
    "\n",
    "WordNet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condaPython3Envs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
